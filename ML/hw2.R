rm(list = ls())
library( ggplot2 )

# 1
set.seed(1)
x <- rnorm(100, mean = 0, sd = 1)
e <- rnorm(100, mean = 0, sd = 0.25)

# RUN FROM THIS POINT TO SEE RESULTS FOR ```y = -1 + 0.5x + e```
y = -1 + 0.5 * x +  e
plot(x, y)
# Obviously a linear relationship
# e is from normal distribution as can be seed in the plot too


df = data.frame(x=x, y=y)
model = lm( y~x, data = df )
summary(model)
confint(model)
# b0-hat = -1.00942       b0 = -1
# b1-hat = 0.49973        b1 = 0.5

ggplot(data = df, aes(x=df$x, y=df$y)) + 
  xlab('X') + ylab('Y') +
  geom_point() +
  geom_abline(aes(intercept = -1, slope = 0.5, colour = 'Original Line'), size=1) +
  geom_abline(aes(intercept = -1.00942, slope = 0.49973, colour = 'Predicted Line'), size=1) +
  labs(colour="Legend") +
  scale_colour_manual(values=c("red", "blue"))



model = lm( y ~ poly(x, 2), data=df)
summary(model)
confint(model)

ggplot(data = df, aes(x=df$x, y=df$y)) + 
  xlab('X') + ylab('Y') +
  geom_point() +
  geom_smooth(method = 'lm', formula =  y ~ poly(x, 2, raw = T)) + 
  labs(colour="Legend") +
  scale_colour_manual(values=c("red", "blue"))
# Yes it improves the model a little bit on the train dataset because of fitting on the
# noise of the dataset generated by the e term, yet there is no evidence that 
# It will perform good enough on more data generated from the same distribution
# Besides the p-value of the x^2 term is high which means it's not significant in the model
# So we may ommit the term when constructing a final model


e = rnorm(100, mean = 0, sd = 0.25)
y = -1 + 0.5 * x +  e
model = lm( y~x, data = data.frame(x=x, y=y) )
confint(model)

e = rnorm(100, mean = 0, sd = 0.5)
y = -1 + 0.5 * x +  e
model = lm( y~x, data = data.frame(x=x, y=y) )
confint(model)

e = rnorm(100, mean = 0, sd = 0.125)
y = -1 + 0.5 * x +  e
model = lm( y~x, data = data.frame(x=x, y=y) )
confint(model)
# Naturally we can see that for error term with smaller standard deviation we get a more
# Confident model compared to the one with higher std





# 2
library(ISLR)
library(corrplot)
df = Auto
model = lm(mpg ~ horsepower, data=df)
summary(model)
ggplot(data = df, aes(x=df$horsepower, y=df$mpg)) + geom_point() + geom_smooth(method='lm') + xlab('Horsepower') + ylab('Miles per gallon')
# As can be seen from both the summary of the model and the plot above
# There is a well defined negative relationship between these two variables
# 0.6 r-squared shows the existance of the relationship

cor(df$horsepower, df$mpg)
# Relationship is pretty high in absolute value -0.7784268 shows strong negative relationship

predict(model, data.frame(horsepower=c(98)))  # 24.46708
confint(model)
#                 2.5 %     97.5 %
#  (Intercept) 38.525212 41.3465103
# horsepower  -0.170517 -0.1451725


plot(df$horsepower, df$mpg)
abline(model,col="pink")


# Residuals vs Fitted values.
plot(model, which=c(1))  # => polynomial relationship 

# Normality
plot(model, which=c(2))
# The normality isn't violated drastically as the points are pretty close
# To the line but the property doesn't hold for the points at the endp

# sqrt(|Standardized Residuals|) vs Fitted values.
plot(model, which=c(3)) 
# equavariance is not satisfied

# Cook's Distance
plot(model, which=c(4)) 
# There are many outliers in the mdoel

# Residuals vs Leverage.
plot(model, which=c(5))



# 3
df = Carseats
model = lm(Sales ~ Price + Urban + US, data = df)
summary(model)
# Multiple R-squared:  0.2393,	Adjusted R-squared:  0.2335 

# P-value is low => we can reject H0: bi = 0
# P-value for Intercept, Price, USYes are low => we can reject H0

model = lm(Sales ~ Price + US, data = df)
summary(model)
# Multiple R-squared:  0.2393,	Adjusted R-squared:  0.2354 

# The new model is a little better in terms of Adjusted R-squared as expected
# Because we excluded an unnecessary variable
# But as we can see both numbers are pretty low which means the linear model doesn't
# Fit the data good enough

confint(model, level = 0.9)
#                     5 %        95 %
# (Intercept) 11.99050157 14.07108394
# Price       -0.06310055 -0.04585472
# USYes        0.77351803  1.62576786

res = resid(model)
plot(df$Sales, 
     res, 
     ylab="Residuals", 
     xlab="Sales") 
abline(0, 0)
# As we can see the errors are not distributed randomly => conditions for BLUE are not satisfied

library(car)
leveragePlots(model)

par(mfrow = c(2, 2))
plot(model)


# Studentized Residuals vs Fitted values
plot(predict(model), rstudent(model))
abline(h = -3, col = 'red')
abline(h = 3, col = 'red')               
which(abs(rstudent(model))>3)
# No outliers

# Leverage statistics. 
hatvalues(model)
plot(hatvalues(model))                   
abline(h=2*2/506, col = 'red')          
which(hatvalues(model)>2*2/506)

# Normality and linearity is satisfied
# Some outliers in present in Residulas vc Leverage plot => model can be improved
# By removing them




# 4
df = Auto
plot(df)
df = df[, !(colnames(df) %in% c("name"))]
cor(df)

corrplot(round(cor(df), 1), method = "circle")

model = lm(mpg ~ ., data = df)
summary(model)
ggplot(data = df, aes(x=df$year, y=df$mpg)) + geom_point() + geom_smooth(method='lm') + xlab('Year') + ylab('Miles per gallon')
# As we can see from the summary most of the variables are significant and the model is
# Pretty good R-squared:  0.8215,	Adjusted R-squared:  0.8182
# Significant variables are: displacement, weight, year, origin
# The cars improve over time and the variables are significant (p-value and r-squared+r-squared-adjusted)


leveragePlots(model)

par(mfrow = c(2, 2))
plot(model)
# Linearity is satisfied 
# QQ plot shows violations at the end for the graph but in the interval [-3; 2] its ok
# Error variance is kind of okay but not perfectly randomly distributed from left to right
# From Residuals vs Leverage plot we can see some outliers present


plot(predict(model), rstudent(model))
abline(h = -3, col = 'red')
abline(h = 3, col = 'red')               
which(abs(rstudent(model))>3)    
# As we can observer from the graph some outliers are present



model = lm(mpg~cylinders*displacement+weight*acceleration+year, data = df)
summary(model)
# Multiple R-squared:  0.8429
# cylinders*displacement is significant
# weight*acceleration is significant

par(mfrow = c(2, 2))
plot(df$weight, df$mpg)
plot(log(df$weight), df$mpg)
plot(df$displacement, df$mpg)
plot(log(df$displacement), df$mpg)
# In both cases log of the weight and displacement contributes to a more linear relationship
# between parameters
